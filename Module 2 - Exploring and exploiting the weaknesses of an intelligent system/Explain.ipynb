{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exploring and exploiting the weaknesses of an intelligent system often involves identifying vulnerabilities in the systemâ€™s design, algorithms, or data handling processes. This can include a variety of techniques such as adversarial attacks, data poisoning, or exploiting flaws in the system's logic.\n",
    "\n",
    "Here is an example focusing on adversarial attacks against a machine learning model:\n",
    "\n",
    "### Example: Adversarial Attacks on Image Classification Model\n",
    "\n",
    "### Objective:\n",
    "To explore and exploit the weaknesses of an image classification model by generating adversarial examples that cause the model to misclassify images.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Use a standard image dataset (e.g., MNIST or CIFAR-10) for training and testing the image classification model.\n",
    "\n",
    "2. **Model Training:**\n",
    "   - Train a Convolutional Neural Network (CNN) on the dataset.\n",
    "\n",
    "3. **Adversarial Example Generation:**\n",
    "   - Use a technique such as the Fast Gradient Sign Method (FGSM) to generate adversarial examples.\n",
    "\n",
    "4. **Evaluation:**\n",
    "   - Evaluate the model's performance on both clean and adversarial examples to understand its weaknesses.\n",
    "\n",
    "### Python Code:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Data Collection\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# Step 2: Model Training\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
    "\n",
    "# Step 3: Adversarial Example Generation using FGSM\n",
    "def generate_adversarial_example(model, image, label, epsilon=0.1):\n",
    "    image = tf.convert_to_tensor(image.reshape((1, 28, 28, 1)))\n",
    "    label = tf.convert_to_tensor(label.reshape((1,)))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model(image)\n",
    "        loss = SparseCategoricalCrossentropy()(label, prediction)\n",
    "    \n",
    "    gradient = tape.gradient(loss, image)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    adversarial_image = image + epsilon * signed_grad\n",
    "    \n",
    "    return adversarial_image.numpy().reshape((28, 28))\n",
    "\n",
    "# Select a sample from the test set\n",
    "sample_image = x_test[0]\n",
    "sample_label = y_test[0]\n",
    "\n",
    "# Generate an adversarial example\n",
    "adversarial_image = generate_adversarial_example(model, sample_image, sample_label)\n",
    "\n",
    "# Step 4: Evaluation\n",
    "# Predict on the clean sample\n",
    "original_pred = np.argmax(model.predict(sample_image.reshape((1, 28, 28, 1))))\n",
    "\n",
    "# Predict on the adversarial sample\n",
    "adversarial_pred = np.argmax(model.predict(adversarial_image.reshape((1, 28, 28, 1))))\n",
    "\n",
    "# Display results\n",
    "print(f\"Original Prediction: {original_pred}\")\n",
    "print(f\"Adversarial Prediction: {adversarial_pred}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(sample_image.reshape((28, 28)), cmap='gray')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Adversarial Image\")\n",
    "plt.imshow(adversarial_image, cmap='gray')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Data Collection:** Loads the MNIST dataset and preprocesses it.\n",
    "- **Model Training:** Trains a simple CNN on the MNIST dataset.\n",
    "- **Adversarial Example Generation:** Uses the Fast Gradient Sign Method (FGSM) to generate adversarial examples that perturb the input image slightly to mislead the model.\n",
    "- **Evaluation:** Compares the model's predictions on clean and adversarial examples and visualizes the original and adversarial images.\n",
    "\n",
    "### Key Points:\n",
    "- **Exploration:** The process of generating adversarial examples explores the weaknesses of the model by identifying how small perturbations in input data can lead to incorrect classifications.\n",
    "- **Exploitation:** The generated adversarial examples exploit these weaknesses to force the model into making incorrect predictions.\n",
    "\n",
    "This example demonstrates how an intelligent system, such as an image classification model, can be both explored and exploited through adversarial attacks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
